{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "369984bc",
   "metadata": {},
   "source": [
    "<span style = 'font-size:1.3em;line-height:1.5em'><b>1. </b>다음 조건을 만족하는 모델 MyNet Class를 작성하시오.</span>\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Input data: </b>(Height, Width) = (224, 224)이며 색상이 있는(RGB 3 channel) Image가 input으로 들어간다. </span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'>(Height, Width) = (224, 224)이며 색상이 있는(RGB 3 channel) Image가 input으로 들어간다. </span>\n",
    "\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>1st conv_batchnorm_relu_pool layers</b></span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>1st conv layer:</b> kernel_size = (3,3), channel수: 32, padding=1, padding_mode='replicate', stride=1</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>1st batch-norm:</b> num_features=32</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Activation function: </b> relu 활성화 함수</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>1st pool layer:</b> kernel_size = (2,2),  </span>\n",
    "\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>2nd conv_batchnorm_relu_pool layers</b></span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>2nd conv layer:</b> kernel_size = (3,3), channel수: 64, padding=1, padding_mode='replicate', stride=1</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>2nd batch-norm:</b> num_features=64</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Activation function: </b> relu 활성화 함수</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>2nd pool layer:</b> kernel_size = (2,2),  </span>\n",
    "\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>3rd conv_batchnorm_relu_pool layers</b></span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>3rd conv layer:</b> kernel_size = (3,3), channel수: 128, padding=1, padding_mode='replicate', stride=1</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>3rd batch-norm:</b> num_features=128</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Activation function: </b> relu 활성화 함수</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>3rd pool layer:</b> kernel_size = (2,2),  </span>\n",
    "\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Flatten: </b> Dense layer(또는 FFNN)의 input으로 들어갈 수 있도록, tensor의 형태 변경</span>\n",
    "\n",
    " \n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>1st dense layer:</b> in_features=<b>??</b>, out_features: 100</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Activation function: </b> relu 활성화 함수</span>\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>2nd dense layer</b> in_features=100, out_features=(class 갯수=2)  </span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d6360677",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b542eb0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "8cb95674",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, in_channels, conv_hidden_channels=[32, 64, 128], hidden_layer=100, n_classes=2):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        # 1st conv_batchnorm_relu_pool layers\n",
    "        self.conv1 = \n",
    "        self.bn1 = \n",
    "        self.pool1 = \n",
    "        \n",
    "        # 2nd conv_batchnorm_relu_pool layers\n",
    "        self.conv2 = \n",
    "        self.bn2 = \n",
    "        self.pool2 = \n",
    "        \n",
    "        # 3rd conv_batchnorm_relu_pool layers\n",
    "        self.conv3 = \n",
    "        self.bn3 = \n",
    "        self.pool3 = \n",
    "        \n",
    "        # FFNN\n",
    "        self.fc1 = \n",
    "        self.fc2 = \n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Conv2d):\n",
    "            nn.init.xavier_normal_(submodule.weight)\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01)\n",
    "        if isinstance(submodule, nn.Linear): # submodule이 nn.Linear에서 생성된 객체(혹은 인스턴스이면)\n",
    "            nn.init.kaiming_normal_(submodule.weight) #해당 submodule의 weight는 He Initialization으로 초기화\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) # 해당 submodule의 bias는 0.01로 초기화\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 1st conv_batchnorm_relu_pool layers에서의 data_flow\n",
    "        out = self.conv1(x)\n",
    "        out = # 1st batchnorm\n",
    "        out = # 1st activation\n",
    "        out = # 1st pooling\n",
    "        \n",
    "        print(\"After 1st conv layer, the shape of the result: \", out.shape)\n",
    "        \n",
    "        # 2nd conv_batchnorm_relu_pool layers에서의 data_flow\n",
    "        out = # 2nd convolution\n",
    "        out = # 2nd batchnorm\n",
    "        out = # 2nd activation\n",
    "        out = # 2nd pooling\n",
    "        \n",
    "        print(\"After 2nd conv layer, the shape of the result: \", out.shape)\n",
    "        \n",
    "        # 3rd conv_batchnorm_relu_pool layers에서의 data_flow\n",
    "        out = # 3rd convolution\n",
    "        out = # 3rd batchnorm\n",
    "        out = # 3rd activation\n",
    "        out = # 3rd pooling\n",
    "        \n",
    "        print(\"After 3rd conv layer, the shape of the result: \", out.shape)\n",
    "        \n",
    "        # Flatten\n",
    "        out = # flatten (or view)\n",
    "        \n",
    "        print(\"After flatten, the shape of the result: \", out.shape)\n",
    "        \n",
    "        out = # 1st dense layer\n",
    "        out = # 1st activation\n",
    "        out = # 2nd dense layer\n",
    "        \n",
    "        print(\"After dense layer, the shape of the result: \", out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d65f03bb",
   "metadata": {},
   "source": [
    "## 다음과 같은 결과가 나와야 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "bfbc7df4",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.randn((16,3,224, 224)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "faaa1a94",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet(in_channels=3).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b2852f0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "After 1st conv layer, the shape of the result:  torch.Size([16, 32, 112, 112])\n",
      "After 2nd conv layer, the shape of the result:  torch.Size([16, 64, 56, 56])\n",
      "After 3rd conv layer, the shape of the result:  torch.Size([16, 128, 28, 28])\n",
      "After flatten, the shape of the result:  torch.Size([16, 100352])\n",
      "After dense layer, the shape of the result:  torch.Size([16, 2])\n"
     ]
    }
   ],
   "source": [
    "out = model(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a21128b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "51e7579f",
   "metadata": {},
   "source": [
    "<span style = 'font-size:1.3em;line-height:1.5em'><b>2. </b>실습 파일 '3.Image_Transformation_and_Augmentation(with torchvision).ipynb'에 있는 cats and dogs 이미지 파일을 활용하여 1번의 모델을 학습하는 코드를 작성하세요. Image transformation은 그림의 크기를 (224,224)로 resize하는 과정은 반드시 포함하고, 나머지는 원하는대로 해도 무관합니다. 또한, 모든 dataset을 training과 evaluate에 사용하세요.</span>\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Hint 1: </b>실습 파일 '3.Image_Transformation_and_Augmentation(with torchvision).ipynb'과 '4.Use_custom_dataset.ipynb'에 있는 ImageFolder내용을 활용하여 CustomDataset을 만들고, 이를 DataLoader에 전달해서 mini-batch 생성기를 만드세요. </span>\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'><b>Hint 2: </b>Train하는데 필요한 코드들은 '2.코드_정리.ipynb'에서 적절히 가지고 오시면 됩니다. 단, CustomDataset과 DataLoader부분은 Hint1의 내용으로 대체하시면 됩니다.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "53d96a15",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, time, pickle\n",
    "import numpy as np\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from torchvision.datasets.folder import ImageFolder"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aef5689",
   "metadata": {},
   "source": [
    "### 모델 정의하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f3682c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyNet(nn.Module):\n",
    "    def __init__(self, in_channels, conv_hidden_channels=[32, 64, 128], hidden_layer=100, n_classes=2):\n",
    "        super(MyNet, self).__init__()\n",
    "        \n",
    "        # 1st conv_batchnorm_relu_pool layers\n",
    "        self.conv1 = \n",
    "        self.bn1 = \n",
    "        self.pool1 = \n",
    "        \n",
    "        # 2nd conv_batchnorm_relu_pool layers\n",
    "        self.conv2 = \n",
    "        self.bn2 = \n",
    "        self.pool2 = \n",
    "        \n",
    "        # 3rd conv_batchnorm_relu_pool layers\n",
    "        self.conv3 = \n",
    "        self.bn3 = \n",
    "        self.pool3 = \n",
    "        \n",
    "        # FFNN\n",
    "        self.fc1 = \n",
    "        self.fc2 = \n",
    "        \n",
    "    def _init_weights(self, submodule):\n",
    "        if isinstance(submodule, nn.Conv2d):\n",
    "            nn.init.xavier_normal_(submodule.weight)\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01)\n",
    "        if isinstance(submodule, nn.Linear): # submodule이 nn.Linear에서 생성된 객체(혹은 인스턴스이면)\n",
    "            nn.init.kaiming_normal_(submodule.weight) #해당 submodule의 weight는 He Initialization으로 초기화\n",
    "            if submodule.bias is not None:\n",
    "                submodule.bias.data.fill_(0.01) # 해당 submodule의 bias는 0.01로 초기화\n",
    "                \n",
    "    def forward(self, x):\n",
    "        \n",
    "        # 1st conv_batchnorm_relu_pool layers에서의 data_flow\n",
    "        out = self.conv1(x)\n",
    "        out = # 1st batchnorm\n",
    "        out = # 1st activation\n",
    "        out = # 1st pooling\n",
    "        \n",
    "        print(\"After 1st conv layer, the shape of the result: \", out.shape)\n",
    "        \n",
    "        # 2nd conv_batchnorm_relu_pool layers에서의 data_flow\n",
    "        out = # 2nd convolution\n",
    "        out = # 2nd batchnorm\n",
    "        out = # 2nd activation\n",
    "        out = # 2nd pooling\n",
    "        \n",
    "        print(\"After 2nd conv layer, the shape of the result: \", out.shape)\n",
    "        \n",
    "        # 3rd conv_batchnorm_relu_pool layers에서의 data_flow\n",
    "        out = # 3rd convolution\n",
    "        out = # 3rd batchnorm\n",
    "        out = # 3rd activation\n",
    "        out = # 3rd pooling\n",
    "        \n",
    "        print(\"After 3rd conv layer, the shape of the result: \", out.shape)\n",
    "        \n",
    "        # Flatten\n",
    "        out = # flatten (or view)\n",
    "        \n",
    "        print(\"After flatten, the shape of the result: \", out.shape)\n",
    "        \n",
    "        out = # 1st dense layer\n",
    "        out = # 1st activation\n",
    "        out = # 2nd dense layer\n",
    "        \n",
    "        print(\"After dense layer, the shape of the result: \", out.shape)\n",
    "        \n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2e5615d",
   "metadata": {},
   "source": [
    "### train() 함수\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>`train()`함수는 각 iteration마다 다음과 같이 진행됩니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 1.</b> batch_loader로부터 mini-batch x, y 데이터를 획득하고 원하는 device에 위치시키기</span>\n",
    "        - <span style = 'font-size:1.0em;line-height:1.5em'> <b>n_data: </b>mini-batch data수, <b>1: </b>channel수(흑백이라서 단일 채널. 칼라 이미지(RGB)는 기본으로 3으로 설정됨) <b>28: </b>Width, <b>28: </b>height</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 2.</b> 지난 batch로부터 계산했던 gradient를 초기화(`zero_grad()`)</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 3.</b> 모델에 batch x를 입력하여 forward propagation</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 4.</b> loss function에 모델이 예측한 각 클래스에 속할 확률(`y_pred_prob`)과 실제 레이블 (`y`)을 넣어서 loss 계산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 5.</b> Backpropagation으로 각 parameter의 gradient를 계산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 6.</b> Gradient Descent로 parameter값 update</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 7.</b> `trn_loss` 변수에 mini-batch loss를 누적해서 합산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 8.</b> 데이터 한 개당 평균 train loss 산출</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "fc1d08b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(model, data_loader, optimizer, criterion, device):\n",
    "    model.train()\n",
    "    trn_loss = 0\n",
    "    \n",
    "    ## 여기에 코드를 작성하세요.\n",
    "    \n",
    "    # Step8. 데이터 한 개당 평균 train loss\n",
    "    avg_trn_loss = trn_loss / len(data_loader.dataset)\n",
    "    return avg_trn_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7623bb8",
   "metadata": {},
   "source": [
    "### evaluate()함수\n",
    "\n",
    "- <span style = 'font-size:1.2em;line-height:1.5em'>`evaluate()`함수는 각 iteration마다 다음과 같이 진행됩니다.</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 1.</b> batch_loader로부터 mini-batch x, y 데이터를 획득하고 원하는 device에 위치시키기</span>\n",
    "        - <span style = 'font-size:1.0em;line-height:1.5em'> <b>n_data: </b>mini-batch data수, <b>1: </b>channel수(흑백이라서 단일 채널. 칼라 이미지(RGB)는 기본으로 3으로 설정됨) <b>28: </b>Width, <b>28: </b>height</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 2.</b> 모델에 batch x를 입력하여 forward propagation</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 3.</b> loss function에 모델이 예측한 각 클래스에 속할 확률(`y_pred_prob`)과 실제 레이블 (`y`)을 넣어서 loss 계산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 4.</b> 모델이 예측하는 레이블을 산출 (with `torch.argmax()`)</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 5.</b> Minibatch의 실제 레이블(`y`)과 예측 레이블(`y_pred_label`)을 누적하여 저장</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 6.</b> `eval_loss` 변수에 mini-batch loss를 누적해서 합산</span>\n",
    "    - <span style = 'font-size:1.1em;line-height:1.5em'><b>Step 7.</b> 데이터 한 개당 평균 evaluation loss와 accuracy 산출</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "22e2467a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate(model, data_loader, optimizer, criterion, device):\n",
    "    model.eval() # 모델을 평가모드로!\n",
    "    eval_loss = 0\n",
    "    \n",
    "    results_pred = []\n",
    "    results_real = []\n",
    "    \n",
    "    # 여기에 코드를 작성하세요.\n",
    "\n",
    "    # Step 7. 데이터 한 개당 평균 eval_loss와 accuracy구하기\n",
    "    avg_eval_loss = eval_loss / len(data_loader.dataset)\n",
    "    results_pred = np.array(results_pred)\n",
    "    results_real = np.array(results_real)\n",
    "    accuracy = np.sum(results_pred == results_real) / len(results_real)\n",
    "    \n",
    "    return avg_eval_loss, accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65a814fa",
   "metadata": {},
   "source": [
    "### 매 Epoch에 드는 시간 측정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "46bf149c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def epoch_time(start_time, end_time):\n",
    "    elapsed_time = end_time - start_time\n",
    "    elapsed_mins = int(elapsed_time / 60)\n",
    "    elapsed_secs = int(elapsed_time - (elapsed_mins * 60))\n",
    "    return elapsed_mins, elapsed_secs"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1997d00a",
   "metadata": {},
   "source": [
    "### Custom Dataset 및 DataLoader생성하기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "0db2297d",
   "metadata": {},
   "outputs": [],
   "source": [
    "image_transform = #여기에 코드를 작성하세요.\n",
    "\n",
    "transform_dset = # 여기에 코드를 작성하세요.\n",
    "\n",
    "trn_loader = # 여기에 코드를 작성하세요.\n",
    "val_loader = # 여기에 코드를 작성하세요."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2661aad9",
   "metadata": {},
   "source": [
    "### 나머지 필요한 parameter 정의하기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54fcd3c8",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>모델 객체 생성하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b7e63e5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = MyNet(in_channels=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a273dbd",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>학습한 모델을 저장할 directory 생성하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "aac3ab73",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_dir = 'models'\n",
    "if not os.path.exists(save_dir):\n",
    "    os.makedirs(save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ef2488cb",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>필요한 hyperparameter값 설정하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "ac13e2b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "N_EPOCHS = 10\n",
    "LR = 2e-4"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c0df9e2",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>loss함수 정의하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "e1fbca42",
   "metadata": {},
   "outputs": [],
   "source": [
    "loss_func = nn.CrossEntropyLoss(reduction='sum')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f84d6347",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>optimizer 생성하기</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "5067d7de",
   "metadata": {},
   "outputs": [],
   "source": [
    "my_opt = optim.Adam(model.parameters(), lr = LR)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31eaecbd",
   "metadata": {},
   "source": [
    "- <span style = 'font-size:1.2em;line-height:1.5em'>trn_data에 대해서 train()함수를, tst_data에 대해서 evaluate()함수를 반복적으로 호출하면서 모델을 학습</span>\n",
    "    - <span style = 'font-size:1.2em;line-height:1.5em'>매 epoch마다 학습이 마무리되면, 모델 평가를 진행한다</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "533740db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\user\\anaconda3\\lib\\site-packages\\PIL\\TiffImagePlugin.py:822: UserWarning: Truncated File Read\n",
      "  warnings.warn(str(msg))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 01 | Time: 37m 2s\n",
      "\tTrain Loss: 0.849 | Test Loss: 0.707 | Test Acc: 58.797% \n",
      "Epoch: 02 | Time: 37m 35s\n",
      "\tTrain Loss: 0.533 | Test Loss: 0.510 | Test Acc: 74.678% \n",
      "Epoch: 03 | Time: 36m 17s\n",
      "\tTrain Loss: 0.471 | Test Loss: 0.450 | Test Acc: 78.210% \n",
      "Epoch: 04 | Time: 36m 23s\n",
      "\tTrain Loss: 0.422 | Test Loss: 0.390 | Test Acc: 82.235% \n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [18]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(N_EPOCHS):\n\u001b[0;32m      5\u001b[0m     start_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n\u001b[1;32m----> 7\u001b[0m     trn_loss \u001b[38;5;241m=\u001b[39m \u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      8\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdata_loader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrn_loader\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m      9\u001b[0m \u001b[43m                     \u001b[49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmy_opt\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\n\u001b[0;32m     10\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mcriterion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m     11\u001b[0m \u001b[43m                     \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     12\u001b[0m     val_loss, accuracy \u001b[38;5;241m=\u001b[39m evaluate(model\u001b[38;5;241m=\u001b[39mmodel, \n\u001b[0;32m     13\u001b[0m                                   data_loader\u001b[38;5;241m=\u001b[39mval_loader, \n\u001b[0;32m     14\u001b[0m                                   optimizer\u001b[38;5;241m=\u001b[39mmy_opt, \n\u001b[0;32m     15\u001b[0m                                   criterion\u001b[38;5;241m=\u001b[39mloss_func,\n\u001b[0;32m     16\u001b[0m                                   device\u001b[38;5;241m=\u001b[39mdevice)\n\u001b[0;32m     18\u001b[0m     end_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime()\n",
      "Input \u001b[1;32mIn [9]\u001b[0m, in \u001b[0;36mtrain\u001b[1;34m(model, data_loader, optimizer, criterion, device)\u001b[0m\n\u001b[0;32m     11\u001b[0m my_opt\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m     13\u001b[0m \u001b[38;5;66;03m# 1-(3): Forward Propagation\u001b[39;00m\n\u001b[1;32m---> 14\u001b[0m y_pred_prob \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     16\u001b[0m \u001b[38;5;66;03m# 1-(4): Loss Calculation\u001b[39;00m\n\u001b[0;32m     17\u001b[0m loss \u001b[38;5;241m=\u001b[39m criterion(y_pred_prob, y)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [8]\u001b[0m, in \u001b[0;36mMyNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     61\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpool2(out)\n\u001b[0;32m     63\u001b[0m \u001b[38;5;66;03m# 3rd conv_batchnorm_relu_pool layers에서의 data_flow\u001b[39;00m\n\u001b[1;32m---> 64\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv3\u001b[49m\u001b[43m(\u001b[49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m out \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn3(out)\n\u001b[0;32m     66\u001b[0m out \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39mrelu(out)\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m forward_call(\u001b[38;5;241m*\u001b[39m\u001b[38;5;28minput\u001b[39m, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:463\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    462\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 463\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\conv.py:456\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    454\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_conv_forward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor, weight: Tensor, bias: Optional[Tensor]):\n\u001b[0;32m    455\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m--> 456\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpad\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_reversed_padding_repeated_twice\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding_mode\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    457\u001b[0m \u001b[43m                        \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    458\u001b[0m \u001b[43m                        \u001b[49m\u001b[43m_pair\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    459\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(\u001b[38;5;28minput\u001b[39m, weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    460\u001b[0m                     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "best_val_loss = float('inf')\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    \n",
    "    start_time = time.time()\n",
    "    \n",
    "    trn_loss = train(model=model, \n",
    "                     data_loader=trn_loader, \n",
    "                     optimizer=my_opt, \n",
    "                     criterion=loss_func,\n",
    "                     device=device)\n",
    "    val_loss, accuracy = evaluate(model=model, \n",
    "                                  data_loader=val_loader, \n",
    "                                  optimizer=my_opt, \n",
    "                                  criterion=loss_func,\n",
    "                                  device=device)\n",
    "    \n",
    "    end_time = time.time()\n",
    "    \n",
    "    epoch_mins, epoch_secs = epoch_time(start_time, end_time)\n",
    "    \n",
    "    if val_loss < best_val_loss:\n",
    "        best_val_loss = val_loss\n",
    "        torch.save(model.state_dict(), f'{save_dir}/my_model5.pt')\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Time: {epoch_mins}m {epoch_secs}s')\n",
    "    print(f'\\tTrain Loss: {trn_loss:.3f} | Test Loss: {val_loss:.3f} | Test Acc: {100*accuracy:.3f}% ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f060faa4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
